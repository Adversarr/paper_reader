# article_processor.py
import os
from typing import List, Optional
from models import ArticleSummary, Content
from utils import (
    ensure_dir_exists,
    slugify,
    save_text_and_embedding,
    load_text_and_embedding,
    split_into_sections
)
from config import (
    DOCS_DIR,
    EXTRACTED_MD_FILE,
    SUMMARIZED_MD_FILE,
    SECTIONS_SUMMARIZED_MD_FILE,
    TLDR_MD_FILE,
    SECTION_SEPARATOR,
    PROMPT_ARTICLE_SUMMARY,
    PROMPT_SECTION_SUMMARY,
    PROMPT_TLDR_SUMMARY,
    PROMPT_EXTRACT_TAGS,
    MAX_TOKENS_SUMMARY,
    MAX_TOKENS_TLDR,
    GPT_MODEL_TAG
)
from openai_utils import get_embedding, generate_completion
from vector_store import get_relevant_context_for_prompt # For potential RAG in summarization

def _generate_and_save_content(
    text_to_process: str,
    prompt_template: str,
    output_dir: str,
    output_filename_md: str,
    max_tokens: int,
    rag_query_for_prompt: Optional[str] = None, # Text to find relevant context for
    previous_content_for_prompt: str = "N/A"
) -> Optional[Content]:
    """Helper to generate, embed, and save a piece of content."""
    existing_content_obj = load_text_and_embedding(output_dir, output_filename_md)
    if existing_content_obj:
        print(f"Found existing {output_filename_md} in {output_dir}, loading.")
        return existing_content_obj

    print(f"Generating {output_filename_md} for {output_dir}...")

    # RAG: Augment prompt with relevant context if query is provided
    rag_context = ""
    if rag_query_for_prompt:
        # Example: get context from other article summaries
        rag_context = get_relevant_context_for_prompt(rag_query_for_prompt, source_type="articles", top_k=2)

    prompt = prompt_template.format(text=text_to_process, previous_summary=previous_content_for_prompt)
    if "{rag_context}" in prompt: # If prompt template supports it
        prompt = prompt.format(text=text_to_process, previous_summary=previous_content_for_prompt, rag_context=rag_context)


    generated_text = generate_completion(prompt, max_tokens=max_tokens)
    if not generated_text:
        print(f"Failed to generate {output_filename_md}.")
        return None

    embedding_vector = get_embedding(generated_text)
    save_text_and_embedding(output_dir, output_filename_md, generated_text, embedding_vector)
    return Content(content=generated_text, vector=embedding_vector)


def process_article(paper_directory_name: str, article_title: str) -> Optional[ArticleSummary]:
    """
    Processes a single article: loads extracted content, generates summaries,
    TLDR, section summaries, and extracts tags. Saves all outputs.
    `paper_directory_name` is the actual directory name on disk (e.g., "paper1").
    `article_title` is the human-readable title used for slugification if needed.
    """
    print(f"\nProcessing article: {article_title} in directory {paper_directory_name}")
    paper_slug = slugify(article_title) # This should match paper_directory_name if generated by this system
    paper_path = os.path.join(DOCS_DIR, paper_directory_name) # Use the provided directory name

    ensure_dir_exists(paper_path)

    # 1. Load original extracted content
    extracted_content_obj = load_text_and_embedding(paper_path, EXTRACTED_MD_FILE)
    if not extracted_content_obj:
        # Try to load just the MD if NPZ is missing, and then embed it
        md_path_only = os.path.join(paper_path, EXTRACTED_MD_FILE)
        if os.path.exists(md_path_only):
            with open(md_path_only, 'r', encoding='utf-8') as f:
                raw_text = f.read()
            print(f"Found {EXTRACTED_MD_FILE}, generating its embedding...")
            embedding_vector = get_embedding(raw_text)
            save_text_and_embedding(paper_path, EXTRACTED_MD_FILE, raw_text, embedding_vector) # This saves .npz
            extracted_content_obj = Content(content=raw_text, vector=embedding_vector)
        else:
            print(f"Error: {EXTRACTED_MD_FILE} not found in {paper_path}. Skipping article.")
            return None
    
    raw_text = extracted_content_obj['content']

    # 2. Generate article-level summary
    # For iterative improvement, we could load an existing summary and pass it to the prompt.
    # Here, we just check if the file exists.
    existing_summary_obj = load_text_and_embedding(paper_path, SUMMARIZED_MD_FILE)
    previous_summary_text = existing_summary_obj['content'] if existing_summary_obj else "N/A"

    summary_obj = _generate_and_save_content(
        text_to_process=raw_text,
        prompt_template=PROMPT_ARTICLE_SUMMARY,
        output_dir=paper_path,
        output_filename_md=SUMMARIZED_MD_FILE,
        max_tokens=MAX_TOKENS_SUMMARY,
        previous_content_for_prompt=previous_summary_text
        # Potentially add rag_query_for_prompt=raw_text[:1000] for broader context
    )

    # 3. Generate TLDR summary
    tldr_obj = _generate_and_save_content(
        text_to_process=raw_text, # Or use summary_obj['content'] for TLDR from summary
        prompt_template=PROMPT_TLDR_SUMMARY,
        output_dir=paper_path,
        output_filename_md=TLDR_MD_FILE,
        max_tokens=MAX_TOKENS_TLDR
    )

    # 4. Generate section-level summaries
    sections = split_into_sections(raw_text, SECTION_SEPARATOR)
    section_summaries_list: List[Content] = []
    all_section_summaries_text = ""

    # Check if SECTIONS_SUMMARIZED_MD_FILE exists to load all section summaries at once
    # This assumes that if the concatenated file exists, individual processing is done.
    # For a more robust check, one might store section summaries individually.
    concatenated_sections_summary_obj = load_text_and_embedding(paper_path, SECTIONS_SUMMARIZED_MD_FILE)

    if concatenated_sections_summary_obj:
        print(f"Found existing {SECTIONS_SUMMARIZED_MD_FILE}, attempting to parse.")
        # This is a simplified load; real implementation might need to parse the file
        # or store section summaries in a more structured way (e.g., JSON with list of Content)
        # For now, we'll re-generate if the main file is missing, or if forced.
        # A better approach would be to save each section summary and its embedding individually.
        # For this minimal version, we'll re-generate if the concatenated file isn't found,
        # and store the concatenation.
        
        # Let's assume if SECTIONS_SUMMARIZED_MD_FILE exists, its content is the concatenated summaries
        # and its .npz is the embedding of that concatenation.
        # To get individual Content objects, we'd need to split and re-embed or store differently.
        # For simplicity, if the main file exists, we'll just use its content for the combined string.
        # And for the List[Content], we'll assume they need to be re-derived or loaded from a different structure.
        #
        # REVISING: We will generate section summaries one by one and save them into the list.
        # Then, we concatenate them and save that with its embedding.
        
        # For this version, we will always process sections if the concatenated file is not present,
        # or if we decide to always re-process sections for finer control.
        # Let's assume for now: if sections_summarized.md exists, we skip individual section processing.
        # This is a simplification. A robust system would manage individual section files.
        
        # Let's re-evaluate: The requirement is `List[Content]` for `section_summaries`.
        # So, we should process each section and store its summary and embedding.
        # `sections_summarized.md` will store the *concatenation* of these summaries.

        # Try to load individual section summaries if they were stored in a specific format.
        # For this minimal version, we'll regenerate them if the main `sections_summarized.md` is missing,
        # implying they haven't been processed and concatenated yet.
        # If `sections_summarized.md` exists, we assume `section_summaries` list needs to be populated
        # by splitting it and re-embedding, or that it's okay to not have individual embeddings for this minimal example.
        #
        # Let's stick to: generate each section summary, get its embedding, add to list.
        # Then, concatenate all section summary texts, save to sections_summarized.md, and embed that.
        pass # Will proceed to generate below

    if not concatenated_sections_summary_obj or not sections: # If no sections or combined file missing
        print(f"Generating section summaries for {paper_directory_name}...")
        for i, section_text in enumerate(sections):
            if not section_text.strip():
                continue
            print(f"  Summarizing section {i+1}/{len(sections)}...")
            # Section summaries usually don't need RAG or previous summaries from other sections.
            section_summary_text = generate_completion(
                PROMPT_SECTION_SUMMARY.format(text=section_text),
                max_tokens=300 # Shorter summaries for sections
            )
            if section_summary_text:
                section_embedding = get_embedding(section_summary_text)
                section_summaries_list.append(
                    Content(content=section_summary_text, vector=section_embedding)
                )
                all_section_summaries_text += section_summary_text + f"\n\n{SECTION_SEPARATOR}\n\n" # Use separator
            else:
                print(f"    Failed to summarize section {i+1}.")
        
        all_section_summaries_text = all_section_summaries_text.strip().strip(SECTION_SEPARATOR).strip()
        if all_section_summaries_text:
            print(f"Saving concatenated section summaries to {SECTIONS_SUMMARIZED_MD_FILE}...")
            concat_embedding = get_embedding(all_section_summaries_text)
            save_text_and_embedding(
                paper_path,
                SECTIONS_SUMMARIZED_MD_FILE,
                all_section_summaries_text,
                concat_embedding
            )
    elif concatenated_sections_summary_obj and sections: # File exists, try to populate list
        print(f"Loading section summaries from {SECTIONS_SUMMARIZED_MD_FILE}...")
        # This is a simplification. We'd ideally split and re-embed or load individual embeddings.
        # For now, we'll split the content and generate embeddings on the fly if needed for the list.
        split_loaded_sections = split_into_sections(concatenated_sections_summary_obj['content'], SECTION_SEPARATOR)
        for sec_sum_text in split_loaded_sections:
            # In a real scenario, you'd load individual embeddings if stored, or re-calculate.
            # For this minimal example, we'll re-calculate if vector is None.
            sec_emb = get_embedding(sec_sum_text) # This could be slow if done often.
            section_summaries_list.append(Content(content=sec_sum_text, vector=sec_emb))
        all_section_summaries_text = concatenated_sections_summary_obj['content']


    # 5. Extract tags
    # Tags are usually extracted once. If re-running, we might want to update them.
    # For simplicity, we'll extract if not already present in a conceptual way.
    # A real system might store tags in the ArticleSummary file or a separate tags.json.
    # Here, we'll just generate them.
    print(f"Extracting tags for {paper_directory_name}...")
    tag_prompt = PROMPT_EXTRACT_TAGS.format(text=raw_text[:4000]) # Use a portion of text for efficiency
    tags_string = generate_completion(tag_prompt, model=GPT_MODEL_TAG, max_tokens=100, temperature=0.2)
    tags_list: List[str] = []
    if tags_string:
        tags_list = [slugify(tag.strip()) for tag in tags_string.split(',') if tag.strip()]
        print(f"  Extracted tags: {tags_list}")
    else:
        print("  Failed to extract tags.")


    article_data = ArticleSummary(
        title=article_title,
        paper_slug=paper_slug,
        content_path=os.path.join(paper_path, EXTRACTED_MD_FILE),
        content=extracted_content_obj,
        summary=summary_obj,
        tldr=tldr_obj,
        section_summaries=section_summaries_list, # List of Content objects for each section summary
        tags=tags_list
    )
    
    # Persist ArticleSummary object? For now, components are saved separately.
    # One could save this TypedDict as a JSON file in paper_path for easy loading.
    # e.g., with open(os.path.join(paper_path, "article_summary_meta.json"), "w") as f:
    #    json.dump(article_data, f, indent=2, cls=NumpyEncoder) # Needs a NumpyEncoder for ndarray

    return article_data
